{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b6f14b-8656-4e44-a6a7-0367b5a224b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f68cdba02f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa55fc8-e04a-460b-93ac-28d2566f3c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 03:58:02 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "df = spark.readStream.format('socket').option('host','localhost').option('port',12345).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5241e1-d881-4aae-9820-099e13700387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d073d7-66f5-48cd-a5f3-c6e739881e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b69e7302-c854-4597-a88c-83467b8d630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nc -l -p 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83a7da71-bb2e-4159-a516-608c279ff144",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df.writeStream.format('console').outputMode('Append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a29615f4-6ca5-4aa9-9d67-1c8c1c2f1644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 03:58:08 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-471d33ec-517c-4ce7-8547-28bee0905184. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/24 03:58:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------+\n",
      "|      value|\n",
      "+-----------+\n",
      "|How Are You|\n",
      "+-----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---------+\n",
      "|    value|\n",
      "+---------+\n",
      "|Iam Happy|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query= writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd798f59-f534-46ce-bcb8-574c3eab4fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b05cbdf2-6953-454f-83ad-2dba1d84e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df.writeStream.format('console').outputMode('Append').trigger(processingTime='5 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebb690e7-fc5a-44b4-87b4-47b24f68ffb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 04:00:30 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1c762603-0c2b-458b-8e84-39a465c80504. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/24 04:00:30 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|Fares|\n",
      "|fares|\n",
      "|fares|\n",
      "+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----------------+\n",
      "|           value|\n",
      "+----------------+\n",
      "|Iam Fares Ashraf|\n",
      "+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|THis Batch of Wor...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bddd0482-7851-4284-930a-c528b929cd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 04:01:04 WARN DAGScheduler: Failed to cancel job group 6b96bdb5-25c9-4fe9-8c42-b77881025346. Cannot find active jobs for it.\n",
      "25/10/24 04:01:04 WARN DAGScheduler: Failed to cancel job group 6b96bdb5-25c9-4fe9-8c42-b77881025346. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7ebb6d3-3f8e-47b5-a0bf-3a32e5747f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a2e118f-ab96-4cf2-aac9-4fcaa67743ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = df.select(split(df['value'],' ').alias('SplittedLine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a83da33-a189-47cb-b5d0-766d1c5843d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SplittedLine: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_split.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "346a2e48-1882-46df-944b-58221e4c0e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df_split.writeStream.format('console').outputMode('Append').trigger(processingTime='5 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89861714-6787-48ae-92d8-c827043c2fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 04:01:25 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ad486b8a-4174-4f7d-8ee6-adfa674d6d0f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/24 04:01:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------------+\n",
      "|SplittedLine|\n",
      "+------------+\n",
      "+------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------+\n",
      "|SplittedLine|\n",
      "+------------+\n",
      "|[Iam, Sad]  |\n",
      "+------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------------+\n",
      "|SplittedLine                 |\n",
      "+-----------------------------+\n",
      "|[This, is, BY, Fares, Ashraf]|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = writer.start(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec4299e7-4c86-48a8-8a67-96c7ad4b9c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 04:01:54 WARN DAGScheduler: Failed to cancel job group 3ff0b0d7-6a39-46fc-b054-1575a31bd19a. Cannot find active jobs for it.\n",
      "25/10/24 04:01:54 WARN DAGScheduler: Failed to cancel job group 3ff0b0d7-6a39-46fc-b054-1575a31bd19a. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1306a8d9-b63b-481f-b815-0efec5c97bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df_split.writeStream.format('console').outputMode('Append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eec808cd-2e86-42e6-bb12-58d7b812a359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 04:02:03 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-368571dd-80e6-41ab-ad20-be2293f50541. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/24 04:02:03 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------------+\n",
      "|SplittedLine|\n",
      "+------------+\n",
      "+------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------+\n",
      "|SplittedLine|\n",
      "+------------+\n",
      "|        [Hi]|\n",
      "+------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------+\n",
      "|SplittedLine|\n",
      "+------------+\n",
      "|[Iam, Fares]|\n",
      "+------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|        SplittedLine|\n",
      "+--------------------+\n",
      "|[No, Waiting, Tim...|\n",
      "+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------------+\n",
      "|SplittedLine|\n",
      "+------------+\n",
      "|    [Thanks]|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26d46861-ed02-407c-aca8-88f92226ffd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 04:02:29 WARN DAGScheduler: Failed to cancel job group 204403b8-7f2f-4798-b07f-6f293677cecb. Cannot find active jobs for it.\n",
      "25/10/24 04:02:29 WARN DAGScheduler: Failed to cancel job group 204403b8-7f2f-4798-b07f-6f293677cecb. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "daecb8f5-f6f2-4e6c-9dff-f3843933712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c71333a2-6aee-408e-bef9-933549c57df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = df_split.select(explode(df_split['SplittedLine']).alias('Words'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87bb9b85-bb29-403c-ad98-b0e4636f9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = df_words.groupBy('words').count()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d488d727-1467-4995-91c3-0f8b10358b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- words: string (nullable = false)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_counts.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4352ab5a-1a78-45a1-b460-4fdb4c8feb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df_counts.writeStream.format('console').outputMode('complete').option('checkpointLocation','ck1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4af3f188-4697-4fb5-8546-5085c7b296ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 04:02:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|words|count|\n",
      "+-----+-----+\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|words|count|\n",
      "+-----+-----+\n",
      "|   by|    1|\n",
      "|words|    1|\n",
      "|   Is|    1|\n",
      "| This|    1|\n",
      "|GRoup|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|words|count|\n",
      "+-----+-----+\n",
      "|   by|    1|\n",
      "|  you|    1|\n",
      "|words|    1|\n",
      "|  iam|    1|\n",
      "|   Is|    1|\n",
      "| meet|    1|\n",
      "|happy|    1|\n",
      "| This|    1|\n",
      "| nice|    1|\n",
      "|GRoup|    1|\n",
      "|   to|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "|  words|count|\n",
      "+-------+-----+\n",
      "|     by|    1|\n",
      "|    you|    1|\n",
      "| thanks|    1|\n",
      "|    for|    1|\n",
      "|  words|    1|\n",
      "|    iam|    1|\n",
      "|watcing|    1|\n",
      "|     Is|    1|\n",
      "|   meet|    1|\n",
      "|  happy|    1|\n",
      "|   This|    1|\n",
      "|   nice|    1|\n",
      "|  GRoup|    1|\n",
      "|     to|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b30d35fa-3a4e-4eb1-8952-caf3fdb33619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 04:06:54 WARN DAGScheduler: Failed to cancel job group ccb0f49b-2013-441d-baee-27731cc83a25. Cannot find active jobs for it.\n",
      "25/10/24 04:06:54 WARN DAGScheduler: Failed to cancel job group ccb0f49b-2013-441d-baee-27731cc83a25. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed827236-3897-4c9d-85c1-6f4e918c9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df_counts.writeStream.format('console').outputMode('complete').option('checkpointLocation','ck1').trigger(processingTime ='1 second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fffe28ea-7940-4068-be9c-ee5f70e6e958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 04:13:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|words|count|\n",
      "+-----+-----+\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 04:13:59 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 17177 milliseconds\n",
      "25/10/24 04:14:18 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 13633 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|words|count|\n",
      "+-----+-----+\n",
      "| Nice|    1|\n",
      "|  you|    1|\n",
      "| meet|    1|\n",
      "|   to|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|words|count|\n",
      "+-----+-----+\n",
      "| Nice|    1|\n",
      "|  you|    1|\n",
      "| meet|    1|\n",
      "|  Iam|    1|\n",
      "| time|    1|\n",
      "|     |    1|\n",
      "|Happy|    1|\n",
      "|Delay|    1|\n",
      "|   to|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 04:14:36 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 17453 milliseconds\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce8427b-51d9-4a3b-b94d-52a957269272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6802d06-2f1b-44a9-bd93-1392c4301600",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62fe5a33-f847-452e-88f0-55f774e696b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3593f13a-910f-418b-81a6-d82500d66c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "369d69fe-8378-4c55-b04b-9251e545f9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7a37a19ac2f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb88fb2c-5b73-4c77-9738-d1aacdde9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =spark.readStream.format('text').load('csv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4011e1a7-bd69-4a04-b0b5-c557738b8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df.writeStream.format('console').outputMode('Append').option('truncate',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e65d1377-cdf9-474f-8e49-3f0f553f58db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 22:40:28 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-cc920f39-29d7-42b7-8067-9a3a15d7bdef. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/25 22:40:28 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------------------------------------------------+\n",
      "|value                                           |\n",
      "+------------------------------------------------+\n",
      "|DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count     |\n",
      "|United States,Romania,1                         |\n",
      "|United States,Ireland,264                       |\n",
      "|United States,India,69                          |\n",
      "|Egypt,United States,24                          |\n",
      "|Equatorial Guinea,United States,1               |\n",
      "|United States,Singapore,25                      |\n",
      "|United States,Grenada,54                        |\n",
      "|Costa Rica,United States,477                    |\n",
      "|Senegal,United States,29                        |\n",
      "|United States,Marshall Islands,44               |\n",
      "|Guyana,United States,17                         |\n",
      "|United States,Sint Maarten,53                   |\n",
      "|Malta,United States,1                           |\n",
      "|Bolivia,United States,46                        |\n",
      "|Anguilla,United States,21                       |\n",
      "|Turks and Caicos Islands,United States,136      |\n",
      "|United States,Afghanistan,2                     |\n",
      "|Saint Vincent and the Grenadines,United States,1|\n",
      "|Italy,United States,390                         |\n",
      "+------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------------------------------------------+\n",
      "|value                                      |\n",
      "+-------------------------------------------+\n",
      "|DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count|\n",
      "|United States,Saint Martin,2               |\n",
      "|United States,Guinea,2                     |\n",
      "|United States,Croatia,1                    |\n",
      "|United States,Romania,3                    |\n",
      "|United States,Ireland,268                  |\n",
      "|Egypt,United States,13                     |\n",
      "|United States,India,76                     |\n",
      "|United States,Singapore,24                 |\n",
      "|United States,Grenada,59                   |\n",
      "|Costa Rica,United States,494               |\n",
      "|Senegal,United States,29                   |\n",
      "|Guyana,United States,26                    |\n",
      "|United States,Marshall Islands,49          |\n",
      "|United States,Sint Maarten,223             |\n",
      "|Malta,United States,1                      |\n",
      "|Bolivia,United States,61                   |\n",
      "|Anguilla,United States,21                  |\n",
      "|United States,Paraguay,3                   |\n",
      "|United States,Gibraltar,1                  |\n",
      "+-------------------------------------------+\n",
      "only showing top 20 rows\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------------+\n",
      "|value                                           |\n",
      "+------------------------------------------------+\n",
      "|DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count     |\n",
      "|United States,Croatia,1                         |\n",
      "|United States,Ireland,252                       |\n",
      "|Egypt,United States,13                          |\n",
      "|United States,India,62                          |\n",
      "|United States,Singapore,25                      |\n",
      "|United States,Grenada,46                        |\n",
      "|Costa Rica,United States,522                    |\n",
      "|Senegal,United States,31                        |\n",
      "|Guyana,United States,65                         |\n",
      "|United States,Marshall Islands,30               |\n",
      "|United States,Sint Maarten,245                  |\n",
      "|Bolivia,United States,35                        |\n",
      "|Anguilla,United States,19                       |\n",
      "|United States,Paraguay,5                        |\n",
      "|United States,Afghanistan,5                     |\n",
      "|Turks and Caicos Islands,United States,183      |\n",
      "|Saint Vincent and the Grenadines,United States,6|\n",
      "|Italy,United States,381                         |\n",
      "|Pakistan,United States,12                       |\n",
      "+------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------------------------------------------+\n",
      "|value                                      |\n",
      "+-------------------------------------------+\n",
      "|DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count|\n",
      "|United States,Romania,12                   |\n",
      "|United States,Croatia,1                    |\n",
      "|United States,Ireland,266                  |\n",
      "|Egypt,United States,13                     |\n",
      "|United States,India,60                     |\n",
      "|Equatorial Guinea,United States,1          |\n",
      "|United States,Niger,1                      |\n",
      "|United States,Singapore,22                 |\n",
      "|United States,Grenada,40                   |\n",
      "|Costa Rica,United States,509               |\n",
      "|Senegal,United States,28                   |\n",
      "|Guyana,United States,34                    |\n",
      "|United States,Sint Maarten,260             |\n",
      "|United States,Marshall Islands,33          |\n",
      "|Bolivia,United States,33                   |\n",
      "|Anguilla,United States,22                  |\n",
      "|United States,Paraguay,15                  |\n",
      "|Algeria,United States,2                    |\n",
      "|Turks and Caicos Islands,United States,181 |\n",
      "+-------------------------------------------+\n",
      "only showing top 20 rows\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------------------------------------------+\n",
      "|value                                      |\n",
      "+-------------------------------------------+\n",
      "|DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count|\n",
      "|United States,Saint Martin,1               |\n",
      "|United States,Romania,12                   |\n",
      "|United States,Croatia,2                    |\n",
      "|United States,Ireland,291                  |\n",
      "|United States,India,62                     |\n",
      "|Egypt,United States,11                     |\n",
      "|United States,Grenada,47                   |\n",
      "|Costa Rica,United States,529               |\n",
      "|Senegal,United States,35                   |\n",
      "|United States,Sint Maarten,290             |\n",
      "|Guyana,United States,52                    |\n",
      "|United States,Marshall Islands,35          |\n",
      "|Malta,United States,2                      |\n",
      "|Malawi,United States,1                     |\n",
      "|Bolivia,United States,33                   |\n",
      "|Anguilla,United States,34                  |\n",
      "|Algeria,United States,9                    |\n",
      "|United States,Paraguay,14                  |\n",
      "|Gibraltar,United States,1                  |\n",
      "+-------------------------------------------+\n",
      "only showing top 20 rows\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-------------------------------------------+\n",
      "|value                                      |\n",
      "+-------------------------------------------+\n",
      "|DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count|\n",
      "|United States,Romania,15                   |\n",
      "|United States,Croatia,1                    |\n",
      "|United States,Ireland,344                  |\n",
      "|Egypt,United States,15                     |\n",
      "|United States,India,62                     |\n",
      "|United States,Singapore,1                  |\n",
      "|United States,Grenada,62                   |\n",
      "|Costa Rica,United States,588               |\n",
      "|Senegal,United States,40                   |\n",
      "|Moldova,United States,1                    |\n",
      "|United States,Sint Maarten,325             |\n",
      "|United States,Marshall Islands,39          |\n",
      "|Guyana,United States,64                    |\n",
      "|Malta,United States,1                      |\n",
      "|Anguilla,United States,41                  |\n",
      "|Bolivia,United States,30                   |\n",
      "|United States,Paraguay,6                   |\n",
      "|Algeria,United States,4                    |\n",
      "|Turks and Caicos Islands,United States,230 |\n",
      "+-------------------------------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3931a883-3caa-4120-ae44-723b772bd35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 22:42:37 WARN DAGScheduler: Failed to cancel job group 8d4b474b-ce36-41a8-8f12-5941c7bf7c72. Cannot find active jobs for it.\n",
      "25/10/25 22:42:37 WARN DAGScheduler: Failed to cancel job group 8d4b474b-ce36-41a8-8f12-5941c7bf7c72. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f79fd651-e37a-45e2-9ab8-caaf8599fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df.writeStream.format('console').outputMode('Append').option('truncate',False).trigger(processingTime='3 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47b050aa-e3c3-48fc-8b54-59616d67630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 22:44:20 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6ad3a466-6874-4e6c-a1d6-6897abba751e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/25 22:44:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------------------------------------------------+\n",
      "|value                                           |\n",
      "+------------------------------------------------+\n",
      "|DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count     |\n",
      "|United States,Romania,1                         |\n",
      "|United States,Ireland,264                       |\n",
      "|United States,India,69                          |\n",
      "|Egypt,United States,24                          |\n",
      "|Equatorial Guinea,United States,1               |\n",
      "|United States,Singapore,25                      |\n",
      "|United States,Grenada,54                        |\n",
      "|Costa Rica,United States,477                    |\n",
      "|Senegal,United States,29                        |\n",
      "|United States,Marshall Islands,44               |\n",
      "|Guyana,United States,17                         |\n",
      "|United States,Sint Maarten,53                   |\n",
      "|Malta,United States,1                           |\n",
      "|Bolivia,United States,46                        |\n",
      "|Anguilla,United States,21                       |\n",
      "|Turks and Caicos Islands,United States,136      |\n",
      "|United States,Afghanistan,2                     |\n",
      "|Saint Vincent and the Grenadines,United States,1|\n",
      "|Italy,United States,390                         |\n",
      "+------------------------------------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a54824a9-ac73-47ac-9000-618eed191264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 22:44:30 WARN DAGScheduler: Failed to cancel job group 19f37c11-0439-43c8-97c2-c8ecd4115442. Cannot find active jobs for it.\n",
      "25/10/25 22:44:30 WARN DAGScheduler: Failed to cancel job group 19f37c11-0439-43c8-97c2-c8ecd4115442. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "18e30b66-61fd-4fdd-9d3e-fee6121ba1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ FILE AS CSV\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as fn \n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55f33780-a1f8-4101-a49d-14b0de181f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "recordSchema = StructType([StructField('date',StringType(),True),\n",
    "                       StructField('delay',IntegerType(),True),\n",
    "                       StructField('distance',IntegerType(),True),\n",
    "                       StructField('origin',StringType(),True),\n",
    "                       StructField('destination',StringType(),True)])\n",
    "\n",
    "\n",
    "df = spark.readStream.format('csv').schema(recordSchema).option('header',True).load('csv/csv2')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5bfb54e4-07a9-4834-8165-d36e95960e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df.writeStream.format('console').outputMode('Append').option('truncate',False).option('NumRows',30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f5304e8-14c0-46ce-94ab-42cdd743984f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 22:58:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-53d95b5e-bf45-4ff8-852c-bd38e7152daf. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/25 22:58:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------+-----+--------+------+-----------+\n",
      "|date    |delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|6    |602     |ABE   |ATL        |\n",
      "|01020600|-8   |369     |ABE   |DTW        |\n",
      "|01021245|-2   |602     |ABE   |ATL        |\n",
      "|01020605|-4   |602     |ABE   |ATL        |\n",
      "|01031245|-4   |602     |ABE   |ATL        |\n",
      "|01030605|0    |602     |ABE   |ATL        |\n",
      "|01041243|10   |602     |ABE   |ATL        |\n",
      "|01040605|28   |602     |ABE   |ATL        |\n",
      "|01051245|88   |602     |ABE   |ATL        |\n",
      "|01050605|9    |602     |ABE   |ATL        |\n",
      "|01061215|-6   |602     |ABE   |ATL        |\n",
      "|01061725|69   |602     |ABE   |ATL        |\n",
      "|01061230|0    |369     |ABE   |DTW        |\n",
      "|01060625|-3   |602     |ABE   |ATL        |\n",
      "|01070600|0    |369     |ABE   |DTW        |\n",
      "|01071725|0    |602     |ABE   |ATL        |\n",
      "|01071230|0    |369     |ABE   |DTW        |\n",
      "|01070625|0    |602     |ABE   |ATL        |\n",
      "|01071219|0    |569     |ABE   |ORD        |\n",
      "|01080600|0    |369     |ABE   |DTW        |\n",
      "|01081230|33   |369     |ABE   |DTW        |\n",
      "|01080625|1    |602     |ABE   |ATL        |\n",
      "|01080607|5    |569     |ABE   |ORD        |\n",
      "|01081219|54   |569     |ABE   |ORD        |\n",
      "|01091215|43   |602     |ABE   |ATL        |\n",
      "|01090600|151  |369     |ABE   |DTW        |\n",
      "|01091725|0    |602     |ABE   |ATL        |\n",
      "|01091230|-4   |369     |ABE   |DTW        |\n",
      "|01090625|8    |602     |ABE   |ATL        |\n",
      "|01091219|83   |569     |ABE   |ORD        |\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 30 rows\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb992ce9-3e7e-4d17-8597-b479f61659fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9de32721-4529-4a22-b8cc-6eab3b203dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 23:00:30 WARN DAGScheduler: Failed to cancel job group 8a1ad1c0-9d64-46ef-a360-876131b77d22. Cannot find active jobs for it.\n",
      "25/10/25 23:00:30 WARN DAGScheduler: Failed to cancel job group 8a1ad1c0-9d64-46ef-a360-876131b77d22. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "874a4cf5-4cbe-49dc-9d15-a9da0682e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = spark.read.parquet('parquet/2010-summary.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9dd41aaa-abed-4e1b-8791-687ee3582ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e9ff728a-2ad5-4aec-89a7-182da284a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "recordSchema = StructType([\n",
    "    StructField('DEST_COUNTRY_NAME', StringType(), False),\n",
    "    StructField('ORIGIN_COUNTRY_NAME', StringType(), False),\n",
    "    StructField('count', LongType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.readStream.format('parquet').schema(recordSchema).load('parquet/2010-summary.parquet/')\n",
    "\n",
    "writer = df.writeStream.format('console').outputMode('Append').option('truncate',False).option('NumRows',30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "07d7d683-550e-4cf4-8a9c-d3a68d61dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e60036c3-7ab2-4aab-a3c1-c06dbfa97e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 23:16:08 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-cd8e48d7-60b3-42ff-a090-4eae9ae51e58. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/25 23:16:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------------------+------------------------------+-----+\n",
      "|DEST_COUNTRY_NAME               |ORIGIN_COUNTRY_NAME           |count|\n",
      "+--------------------------------+------------------------------+-----+\n",
      "|United States                   |Romania                       |1    |\n",
      "|United States                   |Ireland                       |264  |\n",
      "|United States                   |India                         |69   |\n",
      "|Egypt                           |United States                 |24   |\n",
      "|Equatorial Guinea               |United States                 |1    |\n",
      "|United States                   |Singapore                     |25   |\n",
      "|United States                   |Grenada                       |54   |\n",
      "|Costa Rica                      |United States                 |477  |\n",
      "|Senegal                         |United States                 |29   |\n",
      "|United States                   |Marshall Islands              |44   |\n",
      "|Guyana                          |United States                 |17   |\n",
      "|United States                   |Sint Maarten                  |53   |\n",
      "|Malta                           |United States                 |1    |\n",
      "|Bolivia                         |United States                 |46   |\n",
      "|Anguilla                        |United States                 |21   |\n",
      "|Turks and Caicos Islands        |United States                 |136  |\n",
      "|United States                   |Afghanistan                   |2    |\n",
      "|Saint Vincent and the Grenadines|United States                 |1    |\n",
      "|Italy                           |United States                 |390  |\n",
      "|United States                   |Russia                        |156  |\n",
      "|United States                   |Federated States of Micronesia|48   |\n",
      "|Pakistan                        |United States                 |9    |\n",
      "|United States                   |Netherlands                   |570  |\n",
      "|Iceland                         |United States                 |118  |\n",
      "|Marshall Islands                |United States                 |77   |\n",
      "|Luxembourg                      |United States                 |91   |\n",
      "|Honduras                        |United States                 |391  |\n",
      "|The Bahamas                     |United States                 |903  |\n",
      "|El Salvador                     |United States                 |519  |\n",
      "|United States                   |Senegal                       |46   |\n",
      "+--------------------------------+------------------------------+-----+\n",
      "only showing top 30 rows\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "41e26e66-bfd7-4d0d-a2a9-6974e8554733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 23:17:03 WARN DAGScheduler: Failed to cancel job group ab57fe8b-3817-4792-bd26-4ffbf945f2a1. Cannot find active jobs for it.\n",
      "25/10/25 23:17:03 WARN DAGScheduler: Failed to cancel job group ab57fe8b-3817-4792-bd26-4ffbf945f2a1. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4ff5e219-acf7-4342-a4c1-0e9fe1e36ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as fn\n",
    "\n",
    "recordSchema = StructType([\n",
    "    StructField('DEST_COUNTRY_NAME', StringType(), False),\n",
    "    StructField('ORIGIN_COUNTRY_NAME', StringType(), False),\n",
    "    StructField('count', LongType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.readStream.format('parquet').schema(recordSchema).load('parquet/2010-summary.parquet/')\n",
    "\n",
    "df2 = df.groupby('DEST_COUNTRY_NAME').agg(    fn.avg('count').alias('averageCount')   ,   fn.max('count').alias('MaxCount')  )\n",
    "\n",
    "writer = df2.writeStream.format('console').outputMode('Complete').option('truncate',False).option('NumRows',30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c11e69aa-92b3-4ffb-bc9f-8da0da5eca04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 23:20:40 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-acd881f0-15a6-4e2c-93f6-fac75a1f3d7d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/25 23:20:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------------------------+------------+--------+\n",
      "|DEST_COUNTRY_NAME       |averageCount|MaxCount|\n",
      "+------------------------+------------+--------+\n",
      "|Paraguay                |90.0        |90      |\n",
      "|Anguilla                |21.0        |21      |\n",
      "|Russia                  |152.0       |152     |\n",
      "|Senegal                 |29.0        |29      |\n",
      "|Sweden                  |65.0        |65      |\n",
      "|Kiribati                |17.0        |17      |\n",
      "|Guyana                  |17.0        |17      |\n",
      "|Philippines             |132.0       |132     |\n",
      "|Malaysia                |1.0         |1       |\n",
      "|Singapore               |25.0        |25      |\n",
      "|Fiji                    |53.0        |53      |\n",
      "|Turkey                  |75.0        |75      |\n",
      "|Germany                 |1392.0      |1392    |\n",
      "|Afghanistan             |11.0        |11      |\n",
      "|Jordan                  |50.0        |50      |\n",
      "|Palau                   |31.0        |31      |\n",
      "|France                  |774.0       |774     |\n",
      "|Turks and Caicos Islands|136.0       |136     |\n",
      "|Greece                  |50.0        |50      |\n",
      "|Dominica                |28.0        |28      |\n",
      "|British Virgin Islands  |49.0        |49      |\n",
      "|Taiwan                  |275.0       |275     |\n",
      "|Equatorial Guinea       |1.0         |1       |\n",
      "|Slovakia                |1.0         |1       |\n",
      "|Argentina               |184.0       |184     |\n",
      "|Belgium                 |408.0       |408     |\n",
      "|Angola                  |14.0        |14      |\n",
      "|Qatar                   |41.0        |41      |\n",
      "|Ecuador                 |272.0       |272     |\n",
      "|Finland                 |24.0        |24      |\n",
      "+------------------------+------------+--------+\n",
      "only showing top 30 rows\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a1b8e40f-fe39-4842-8d9c-5689e523088f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 23:33:52 WARN DAGScheduler: Failed to cancel job group daae2ae6-009f-4692-9887-7be47531d16b. Cannot find active jobs for it.\n",
      "25/10/25 23:33:52 WARN DAGScheduler: Failed to cancel job group daae2ae6-009f-4692-9887-7be47531d16b. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b3373509-9e22-4576-a611-29ab463d8225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Files Only WIth Append Mode Only\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as fn\n",
    "\n",
    "recordSchema = StructType([\n",
    "    StructField('DEST_COUNTRY_NAME', StringType(), False),\n",
    "    StructField('ORIGIN_COUNTRY_NAME', StringType(), False),\n",
    "    StructField('count', LongType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.readStream.format('parquet').schema(recordSchema).load('parquet/2010-summary.parquet/')\n",
    "\n",
    "df2 = df.groupby('DEST_COUNTRY_NAME').agg(    fn.avg('count').alias('averageCount')   ,   fn.max('count').alias('MaxCount')  )\n",
    "\n",
    "writer = df2.writeStream.format('memory').outputMode('Complete').queryName('MyTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a783d845-a781-4110-8ff6-98c957218133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 23:44:10 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a1178590-ca17-4b3f-80d2-6bfd64d73118. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/25 23:44:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "de84e0ff-4ee6-4461-a17c-7c792e11fd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql('select * from MyTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "543cfdbc-398e-451b-8c20-5fe03be3fad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------+\n",
      "|   DEST_COUNTRY_NAME|averageCount|MaxCount|\n",
      "+--------------------+------------+--------+\n",
      "|            Paraguay|        90.0|      90|\n",
      "|            Anguilla|        21.0|      21|\n",
      "|              Russia|       152.0|     152|\n",
      "|             Senegal|        29.0|      29|\n",
      "|              Sweden|        65.0|      65|\n",
      "|            Kiribati|        17.0|      17|\n",
      "|              Guyana|        17.0|      17|\n",
      "|         Philippines|       132.0|     132|\n",
      "|            Malaysia|         1.0|       1|\n",
      "|           Singapore|        25.0|      25|\n",
      "|                Fiji|        53.0|      53|\n",
      "|              Turkey|        75.0|      75|\n",
      "|             Germany|      1392.0|    1392|\n",
      "|         Afghanistan|        11.0|      11|\n",
      "|              Jordan|        50.0|      50|\n",
      "|               Palau|        31.0|      31|\n",
      "|              France|       774.0|     774|\n",
      "|Turks and Caicos ...|       136.0|     136|\n",
      "|              Greece|        50.0|      50|\n",
      "|            Dominica|        28.0|      28|\n",
      "+--------------------+------------+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d18a7cf6-415c-446a-ae9f-ac8e49b5d84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 23:47:55 WARN DAGScheduler: Failed to cancel job group 39663778-5255-4694-966a-637c38988d00. Cannot find active jobs for it.\n",
      "25/10/25 23:47:55 WARN DAGScheduler: Failed to cancel job group 39663778-5255-4694-966a-637c38988d00. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1b5d896f-ff3f-4c65-b2f4-e8e29a424a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Files Only WIth Append Mode Only\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as fn\n",
    "\n",
    "recordSchema = StructType([\n",
    "    StructField('DEST_COUNTRY_NAME', StringType(), False),\n",
    "    StructField('ORIGIN_COUNTRY_NAME', StringType(), False),\n",
    "    StructField('count', LongType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.readStream.format('parquet').schema(recordSchema).load('parquet/2010-summary.parquet/')\n",
    "\n",
    "writer = df.writeStream.format('memory').outputMode('append').queryName('MyTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "60e5e9c7-fa65-4330-a1a9-8b32fc16c659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 23:49:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-069ef92b-2578-45b5-86a7-b1f70439f95c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/25 23:49:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "eee59e0a-1bea-4819-aef9-591ccfc96975",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql = spark.sql('select * from MyTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b6312c62-7e29-4a38-b942-9ae9903451f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1093c9d9-1609-4e26-8823-dd4b72f36b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 23:55:58 WARN DAGScheduler: Failed to cancel job group 04ae4b21-24b7-4970-9d0d-14efd7fcb2d8. Cannot find active jobs for it.\n",
      "25/10/25 23:55:58 WARN DAGScheduler: Failed to cancel job group 04ae4b21-24b7-4970-9d0d-14efd7fcb2d8. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fae19f37-4319-4a60-9492-f97b8307121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format('parquet').schema(recordSchema).load('parquet/2010-summary.parquet/')\n",
    "\n",
    "writer = df.writeStream.format('parquet').outputMode('append').option('checkpointLocation','Stream_chk').option('path','Saved_Stream_Parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "01e1cf94-9525-4c27-8946-2bca5c9ba2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 23:59:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "query =writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a28c7788-3ab2-4af8-8603-7ae632363d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/26 00:08:18 WARN DAGScheduler: Failed to cancel job group 80e2ab8c-94b6-412a-9498-673199037c69. Cannot find active jobs for it.\n",
      "25/10/26 00:08:18 WARN DAGScheduler: Failed to cancel job group 80e2ab8c-94b6-412a-9498-673199037c69. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1f2d367e-2848-495a-8f26-6899f9e0ac08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist() # remove from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2658f175-f3a5-44eb-86f0-33c7ab81e513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
